\chapter{Fazit}
\label{chap:fazit}

% fazit in die loesung packen? Fazit = Fazit zum Vergleich der Tools und der eingesetzetn Architektur
% architektur gut weil
% kafka hat ALLE Daten
% postgres nur daten mit sinn!
% macht kein sinn auf unserer architektur aber definitv big data
% Obwohl unsere konkrete Implementierung
% kafka connectoren wäre besser
% jupyter vs. zeppelin feature vergleich
% java vs. python vs. kafka stream vs. kafka connect

Apache Kafka wird dazu verwendet um alle empfangenen Datensätz ungeachtet ihre Inhaltes in einem Topic zu speichern.
Für dieses Projekt relevante Informationen wurden dann in die PostgreSQL Datenbank übertragen und konnten dann von dort mittels
Apache Zeppelin und Jupyter angerufen werden.

Aufgrund der limitiertes Grüße unseres Datensatzes und der überschaubaren Rechenleistung des verfügbaren Equipments in unserem Projekt
hätte auf Apache Kafka verzichtet werden können, sodass die Daten direkt aus der \ac{API} in die Datenbank geschrieben werden
ohne den \glqq Umweg\grqq{} über Apache Kafka.\\
Jedoch in einem realen Szenario ist dieser Weg über Apache Kafka keineswegs falsch.\\
Dieser Ansatz ermöglicht es zum Speicherplatz in der Datenbank zu sparen und trägt zu einer besseren Performance bei.
Darüber hinaus kann mit der Verwendung von Kafka Streams das original Topic gezielter angepasst und/oder verändert werden.

Kafka Streams ist eine Klient Bibliothek zur Verfügung gestellt von Confluent.
Mit dieser Bibliothek können Anwendungen und Microservices erstellt werden, die eingehende Daten eines Topics
auslesen, weiterverarbeiten und in zweites und vor allem separates Topic schreiben.

Kafka Streams bieten den Vorteil, dass sie komplett in das vorhandenen Apache Kafka Ökosystem integriert,
hoch skalierbar und fehlertolreant sind.\\
Die dafür empfohlenen Programmiersprache von Konfluent ist Java,
jedoch gibt es auch Implementierungen für andere Sprachen wie \zb{} Python.\autocite{KafkaStreams}

Beim Vergleich der beiden Implementierung für jeweils Kafka Producer und Kafka Consumer fällt auf, dass die Java Implementierung eher mit low-level APIs operiert und damit mehr Aufwand für Konfiguration und Konvertierung von Daten anfällt, wohingegen Python durch Verwendung von Frameworks, die Komplexität stärker abstrahieren, und schlankerer Syntax mit deutlich weniger Codezeilen auskommt. Somit stellt Python für einen PoC bzw. Prototypen die schnellere Alternative da, wobei Java und sein feinerer Detailgrad eher in komplexeren Umgebungen seine Stärken ausspielen kann.\newline
Aufgrund dessen, dass die Komplexität unserer Consumer und Producer insgesamt gering ist, da keine inhaltliche Transformation der Daten stattfindet, empfiehlt es sich für den produktiven Betrieb eher Kafka Connector zu evaluieren. Kafka Connector sind ein  Teil der Confluent-Plattform und bilden Connectoren zu gängigen Datenquellen und -senken. Dabei setzen sie auf deklarative Konfiguration anstatt individuelle und imperative Programmskripte. Zudem bieten sie Fehlertoleranz und bessere Skalierbarkeit als einzelne Consumer bzw. Producer Skripte.

Obwohl Apache Zeppelin im Mai 2016 den Incubator Status verlies macht es im allgemeinen einen noch sehr unreifen und unfertigen Eindruck,
was bei Versionsnummer 0.7 nichts überraschendes ist.
Bemerkbar macht sich dies vor allem bei der Installation und Setup insbesondere unter Windows.
So glückte der Start von Zeppelin auf nur 1 von 3 Windows PCs.
Des weiteren stürze der JDBC Interpreter während des Betriebs mehrfach aus unerfindlichen Gründen ab, was nur durch einen Neustart behoben werden konnte.
Zudem bietet Zeppelin 0.7 leider nur sehr rudimentäre Charting-Funktionalitäten.
Zwar gibt es eine Vielzahl an externen Plug-Ins die via Zeppelins Erweiterungsframework \textit{Helium} eingebunden werden können und erweiterte Diagrammtypen\footnote{z.B. Kartendienste und  Heatmaps, siehe \href{https://zeppelin.apache.org/helium_packages.html}{https://zeppelin.apache.org/helium\_packages.html} } bereitstellen, doch setzen diese Plug-Ins oft Zeppelin Version 0.8-\textit{SNAPSHOT} voraus. Diese Entwicklungsversion kann man nur per Sourcecode beziehen und selber kompilieren bzw. bauen.
Das klappte sogar nach mehreren Versuchen, jedoch ließ sich das fertige \textit{binary} nicht starten.\newline
Letztlich lässt sich zusammenfassen, dass die Konzepte hinter Apache Zeppelin gut sind, die konkrete Implementierung jedoch noch nicht ausgereift ist.

Die Installation und Erstkonfiguration mit Jupyter unter Windows ist in wenigen Minuten erledigt.
Sobald Jupyter und die \text{Magic-Commands} mit dem Paketmanager \textit{pip} installiert wurden können
schon erste \ac{SQL} Statements abgesetzt werden.

Resultate eines \ac{SQL} Statements werden grafisch als Tabelle dargestellt.
Für jegliche Art von Visualisierung die darüber hinaus geht müssen separate Bibliotheken installiert werden
über die dann die Visualisierung erfolgt.
Zusätzlich müssen \textit{Widgets} installiert und aktiviert werden wenn die Grafik integriert in Jupyter angezeigt werden soll.\\
Auch hier gibt es Einschränkungen:\\
Falls kein Widget für die aktuell genutzte Bibliothek zur Verfügung steht kann nur die Anzeigemöglichkeiten der Bibliothek genutzt werden.

Das dynamische Ändern eines \ac{SQL} Statements mit einem Textfeld - so wie es in Apache Zeppelin erfolgen kann - benötigt ihn Jupyter zwar zusätzlichen Implmentierungsaufwand kann aber letztlich genauso wie in Apache Zeppelin erfolgen.

Die \textit{Magic-Commands} sind sowohl in Apache Zeppelin als auch Jupyter eine sehr gute Möglichkeit um kleine Code Snippets mit \code{\ac{SQL}} oder \code{R} zu Prototypen ohne tief in die Programmierung einzusteigen.
Alles darüber hinaus wie komplexe Visualiserungen oder Manipulation der Ergebnisse erfordert die Nutzung von weiteren Bibliotheken und
Visualisierung können nur bedingt in Apache Zeppelin und Jupyter angezeigt werden.

In unserem Projekt haben wir die Erkenntnis gewonnen, dass die virtuellen Notebooks für schnelles Prototyping sehr gut geeignet sind.
Will man aber das Notebook so anpassen, dass \zb{} dynamisch Daten von einer externen Datenquelle abgerufen werden
und sich die genutztes Charts kontinierlich anpassen - Stichwort: Monitoring - dann ist wesentlich mehr Aufwand nötig und man stellt sich die
Frage ob die virtuellen Notebooks dafür noch geeignet sind und nicht für dieses Szenario ein anderes oder weitere Tools benötigt werden.

Die Erstellung eines virtuellen Notebooks das zu Monitoringzwecken genutzt werden kann birgt weitere interessante Ansätze und Überlegungen die
jedoch, mit Hinblick auf das Projektziel, nicht weiter verfolgt wurden.
