\subsection{Data Ingestion via \acs{SODA} Schnittstelle}\label{subsec:api}

Folgende Python Bibliotheken wurden genutzt um die Implementierung des Producers zu unterstützen:

\begin{itemize}
  \item \code{sodapy}
  \item \code{kafka-python}
\end{itemize}

Im Kern basiert \code{sodapy} auf dem \code{Request} Paket von Python und vereinfacht das Absenden von Anfragen an die \ac{SODA} \ac{API}.\autocite{Sodapy}

\code{kafka-python} ist ein offiziell unterstützter Klient für Apache Kafka in der Programmiersprache Python und basiert grob auf der offiziellen Java Implementierung des Kafka Klients.\autocite{KafkaPython}
Gegenüber dem offiziellen Paket \code{confluent-kafka-python} ist \code{kafka-python} komplett in Python geschrieben und somit entfällt die Installation des Pakets \code{librdkafka},
das bei dem offiziellen Klient von Confluent noch installiert werden muss.\autocite{KafkaClients}
\newpage
\subsubsection{Python Quellcode Programmablauf}
Der Producer besteht insgesamt aus zwei Python Skripten:

\begin{itemize}
  \item \code{SodaHelper.py}
  \item \code{producer.py}
\end{itemize}

Das SodaHelper Skript ist ein separater Wrapper um die \code{sodapy} Bibliothek um die Verbindung zu der \ac{API} herzustellen und die Daten zu holen.
\\
Der Producer ist für die Weiterleitung der empfangenen Daten an Apache Kafka zuständig.
\\
Somit wird eine klare Aufgabentrennung erreicht.

Da es sich hierbei nicht um ein Live Stream handelt wie \zb{} bei der Twitter \ac{API}, haben wir uns dazu entschieden einen \glqq Fake Stream\grqq{} zu erstellen.
\\
Dies wurde erreicht indem nicht alle Daten sofort an Apache Kafka weitergeleitet werden sondern immer ein Abstand zwischen 0 und 1 Sekunde zwischen dem Senden der einzelnen Datensätze
erzwungen wird.

Der ausgewählter Datensatz ist sehr groß.
Wenn \zb{} der Nutzer Daten von einem Monat abrufen will kann es vorkommen,
dass in diesem Zeitraum mehr als 100.000 Datensätze geladen werden.

Da der Abruf von einer solch großen Menge an Daten über die API eine sehr hohe Rechenleistung erfordert und diese im Rahmen des Projektes nicht zur Verfügung steht wurde
ein Paging entwickelt.

Durch die feste Angabe eines Limits von 10.000 wird sichergestellt, dass ein Request an die \ac{SODA} \ac{API} nur maximal 10.000
Datensätze liefern kann und der Algorithmus führt so lange Requests an die \ac{API} aus bis der gewünschte Zeitraum  komplett empfangen und
die Request an Apache Kafka gesendet wurden.

Vorgegeben durch das Paket \code{kafka-python} können die Einträge eines Topics nur als Byte String abgelegt werden.
Aus diesem Grund wird der empfangende Datensatz zuerst in ein Byte String umgewandelt und dann an Apache Kafka gesendet.

\fullref{lst:csp} zeigt den Algorithmus um das Paging zu realisieren.

Mit Hilfe des SodaHelpers werden zunächst die Datensätze von der \ac{API} - unter Berücksichtigung des Limits, des Anfangs- und Enddatums geholt.
\\
Die Variablen \code{from\_date} und \code{to\_date} werden beim Starten des Programms von dem User gesetzt.

Der Algorithmus beruht auf der Annahme, dass wenn der empfangene Datensatz genau die Länge des Limits hat es immer noch weitere Datensätze gibt die von der \ac{API} abgerufen werden müssen.
Wenn also das Limit erreicht wurde wird das Datum des letzten Datensatzes als neues Enddatum festgelegt und der Prozess beginnt von vorne.
\\
Dies geschieht solange die Anzahl der empfanenen Datensätze nicht dem Limit entsprechen
oder die verwendeten Anfangs- und Enddatumswerte identisch sind.

\lstinputlisting[label=lst:csp,language=Python, caption=Code Snippet aus Producer.py, firstline=23, lastline=52]{../python/Producer.py}
