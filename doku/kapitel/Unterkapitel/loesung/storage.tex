\section{Data Storage}
Apache Kafka wird dazu verwendet um den kompletten Datensatz eines Service Requests abzuspeichern.
Ein Consumer liest die Daten von Apache Kafka aus und speichert die relevanten Attribute eines Datensatzes in der Datenbank.
Als Datenksystem haben wir in unserem Projekt für PostgreSQL entschieden.

Zunächst erstellten wir eine Tabelle mit dem Namen \textitbf{service\_request} um die Service Requests von New York zu speichern.
Eine stichprobenartige Analyse des Datensatzes hat ergeben, dass einzelne Felder des original Datensatzes sehr häufig mit \code{NULL}
Werten belegt sind.
In der Datenbanktabelle \textitbf{service\_request} wurden diese Felder außer Acht gelassen.

Die Datenbanktabelle besitzt folgende Attribute
\begin{enumerate}
  \item unique\_key
  \item created\_date
  \item agency\_name
  \item complaint\_type
  \item descriptor
  \item longitude
  \item latitude
  \item agency
  \item location\_type
  \item incident\_zip
  \item incident\_address
  \item street\_name
  \item cross\_street\_1
  \item cross\_street\_2
  \item address\_type
  \item city
  \item status
  \item due\_date
  \item borough
  \item resolution\_description
\end{enumerate}

In der offiziellen Dokumentation des Datensatzes werden die einzelnen Attribute
eines Service Requests genau beschrieben und deren technische Bezeichner aufgelistet.
\footnote{https://dev.socrata.com/foundry/data.cityofnewyork.us/fhrw-4uyv}

Die Attribute der Tabelle \textitbf{service\_request} sind identisch zu den
Bezeichnern des original \ac{SODA} Datensatzes.

Um das Handling mit Aoache Kafka zu vereinfachen wurden zwei weitere Python
Bibliotheken installiert.
Diese sind:

\begin{itemize}
  \item kafka-python
  \item sqlalchemy
  \item psycopg2 (wird von sqlalchemy benötigt)
\end{itemize}

Genau wie bei dem Producer abstrahiert das Paket \code{kafka-python} die Verbindung zu unserem Apache Kafka Server und erleichtert den Zugriff auf das Topic.
Durch den Einsatz von \code{sqlalchemy} ist es möglich auf die Datenbank und deren Tabelle(n) in einer objektorientierten Weise zuzugreifen
und die Ausführung von \ac{SQL} Statements wird vereinfacht bzw. durch Klassenmethoden abstrahiert.

\subsubsection{Python Quellcode Programmablauf}
\label{subsub:quellcode_storage}
Neben den verwendeten Bibliotheken besteht derConsumer aus zwei Python Skripten.

\begin{itemize}
  \item DBHelper.py
  \item consumer.py
\end{itemize}

Während sich das \code{consumer} Skript um das Auslesen eines Kafka Topics kümmert ist die \code{DBHelper} Klasse
für die Verbindung zu der Datenbank zuständig, liest Tabellenspalten aus und speichert einen Datensatz in der Tabelle.

Nachfolgend die Code Snippet der Consumer Klasse.

\lstinputlisting[language=Python, firstline=13, lastline=27]{../python/Consumer.py}

Sobald das Consumer Skript aufgerufen wird, wird eine Verbindung mit der Datenbank
aufgebaut und der \code{KafkaConsumer} stellt eine Verbindung mit dem Apache Kafka
Server bzw. dem Topic "ServiceRequest" her.
(Zeile 1 - 9)

Sobald von seitens des Producers ein neuer Datensatz in das Topic "ServiceRequests" geschrieben wird,
wird der Datensatz ausgelesen in ein \ac{JSON} umgewandelt, die benötigten Attribute aus dem \ac{JSON} gelesen
und in ein temporäres Dictionary geschrieben.
Dieses Dictionary wird abschließend mit Hilfe des \code{DBHelper} Skripts in die
Datenbanktabelle geladen.
(Zeile 11 - 18)

Wie eingangs erwähnt bezeichneten wir die Attribute der Datenbanktabelle und des \ac{SODA} Datensatzes identisch.

Mit diesem kleinen \glqq Kniff\grqq{} kann mit Hilfe der Namen der Tabellenspalten
auf die Keys des \ac{JSON} zugegriffen, den zugehörigen Wert ausgelesen und als neuen Wert für das temporäre Dictionary genutzt werden.
(Zeile 14 - 16)

\subsubsection{Java Quellcode Programmablauf}
Auch für den Consumer gibt es eine alternative Implementierung in Java.
Diese findet sich in \code{com.srh.bdba.dataengineering.MyConsumer}.
Der Programmablauf ist ähnlich zu der Python Implementierung, weshalb an dieser Stelle auf Codelistings im Anhang TODO verwiesen wird.
Kurz zusammengefasst wird ein \code{KafkaConsumer<Long, String>} konfiguriert und pro 100 Millisekunden am Kafka-Cluster nachgefragt, ob es neue \code{ConsumerRecord <Long, String>} für das entsprechende Topic gibt.
Falls dem so ist wird die \ac{JSON} Nachricht aus dem \code{ConsumerRecord<Long, String>} ausgepackt und per \code{PreparedStatement} über \code{JDBC} in die PostgreSQL Tabelle eingefügt.
