\chapter{Einleitung}
\label{chap:einleitung}

\section{Aufgabe und Ziel}
Im Rahmen dieses Projektes war es die Zielstellung sich mit Data Ingestion, Data Storage sowie Data Retrieval vertraut zu machen.
\paragraph{Data Ingestion} ist die Beschaffung der Daten.
Dies kann entweder mit Hilfe eines Data Streams erfolgen oder einer statischen Datenquelle - also eine einfache Datei die lokal
auf einem Rechner angelegt wird und Daten beinhät wie \zb{} eine \ac{CSV} oder\ac{JSON} Datei.
Unter einem Data Stream versteht mann einen kontinuierlichen Datenstrom wie \zb{}
die Erstellung von immer wieder neuen Twitter Nachrichten.
Ein wichtiges Merkmal eines Data Streams ist, dass man nicht vorhersehen kann wann der Datenstom zu Ende ist - er könnte theoretisch unendlich sein.
Im Falle von eines Datenstroms von Twitter Nachrichten ist es nicht abzusehen wann jemals die letzte Twitter Nachricht geschrieben wird.
Für unsere Aufgabe ist darauf zu achten, dass der Datenstrom über eine API öffentlich zugänglich ist und immer auf dem aktuellsten Stand gehalten wird.
\paragraph{Data Storage} ist die Speicherung der Daten.
Hierbei wurde uns lediglich die Anforderung gestellt, das wir für die Speicherung die Streaming Plattform Apache Kafka verwenden.
Des Weiteren war es uns gestattet die Daten in einer relationen Datenbank, NoSQL Datenbank oder mit Spark Streaming speichern,
falls nur die Nutzung von Apache Kafka unsere Anforderungen nicht genügt.
\paragraph{Data Retrieval} ist die Beschaffung der Daten aus einer Datenbank mit SQL Abfragen und die abschließende Ausgabe
der Ergebnisse in Form von Tabellen oder einfachen Visualisierungen.
Es sollten mindestens drei verschieden SQL Abfragen abgesetzt werden mit unterschiedlichen Filter- und Agreggatsfunktionen sowie einer Teilaggregation wie \zb{} GROUP BY.
Die Visualisierung der Daten sollte in einem virtuellen Notebook erfolgen.
Als virtuelles Notebook durften wir uns entscheiden zwischen Apache Zeppelin oder Jupyter.

Unsere Aufgabe ist es eine geeignete Vorgehensweise für die Bewältigung dieser Aufgabe zu finden und umzusetzen.

Wir entschieden uns für unser Szenario Daten von NYC Open Data zu nutzen.
NYC Open Data ermöglicht es allen "New Yorkern" und somit auch der ganzen Welt sog. Open Data also frei zugängliche Daten
einfach zu konsumieren.\autocite{NYCOpenData}.

NYC Open Data ermöglicht es uns sowohl einen kontinuierlichen Data Stream als auch eine statische \ac{CSV} Datei zu konsumieren.
Dank diesem Umstand entschieden wir uns im Rahmen dieses Projektes beide Möglichkeiten umzusetzen und zu vergleichen.
Auch bei der Data Retrieval entschieden wir uns dafür sowohl Apache Zeppelin als auch Jupyter zu nutzen und zu vergleichen.

\fullref{chap:tools} beschäftigt sich detailierter mit den verwendeten Tools, Frameworks und Programmiersprachen und \fullref{chap:loesung}
erläutert das gewählte Szenario mit den Unterkapitel Data Ingestion, Data Storage und Data Retrieval sowie der verwendeten Architektur.
